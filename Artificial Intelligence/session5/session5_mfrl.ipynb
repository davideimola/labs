{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB SESSION 5: Model-Free Reinforcement Learning\n",
    "\n",
    "In this tutorial we will see some additional functionalities available to OpenAI Gym environments\n",
    "\n",
    "## Cliff environment\n",
    "\n",
    "The environment used is **Cliff** (taken from the book of Sutton and Barto as visible in the figure)\n",
    "![CliffWalking](images/cliff.png)\n",
    "\n",
    "The agent starts in cell $(3, 0)$ and has to reach the goal in $(3, 11)$. Falling from the cliff resets the position to the start state (the episode ends only when the goal state is reached). All other cells are safe. Action dinamycs is deterministic, meaning that the agent always reaches the desired next state.\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "Your first assignment is to implement the *Q-Learning* algorithm on **Cliff**. In particular you need to implement both $\\epsilon$-greedy and *Softmax* versions for the exploration heuristic. The solution returned must be a tuple *(policy, rewards, lengths)* where:\n",
    "* *policy*: array of action identifiers where the $i$-th action refers to the $i$-th state\n",
    "* *rewards*: array of rewards where the $i$-th reward refers to the $i$-th episode of the training performed\n",
    "* *lengths*: array of lengths where the $i$-th length refers to the $i$-th episode of the training performed (length in number of steps)\n",
    "\n",
    "Functions to implement:\n",
    "* *epsilon_greedy(q, state, epsilon)*\n",
    "* *softmax(softmax(q, state, temp)*\n",
    "* *q_learning(environment, episodes, alpha, gamma, expl_func, expl_param)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym\n",
    "import envs\n",
    "import numpy as np\n",
    "from utils.funcs import run_episode, plot, rolling\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to draw a number given a specific probability distribution. In particular, among the 5 choices, the 3rd is the one that is most likely to be chosen (highest probability value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5, p=[0.1, 0.2, 0.5, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions have to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    bestAction = np.argmax(q[state])\n",
    "    if np.random.uniform() <= epsilon:\n",
    "        bestAction = np.random.choice(len(q[state]))\n",
    "    return bestAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(q, state, temp):\n",
    "    \"\"\"\n",
    "    Softmax action selection function\n",
    "    \n",
    "    Args:\n",
    "    q: q table\n",
    "    state: agent's current state\n",
    "    temp: temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    p = np.exp(q[state]/temp)\n",
    "    pSum = np.sum(p)\n",
    "    p = np.divide(p,pSum)\n",
    "    return np.random.choice(len(q[state]),p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(episodes)  # Rewards array\n",
    "    lengths = np.zeros(episodes)  # Lengths array\n",
    "    policy = np.random.choice(env.action_space.n, env.observation_space.n)  # Random policy\n",
    "    #Genero Q\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    for e in range(episodes):\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            a = expl_func(Q,s,expl_param)\n",
    "            nextState,r,d, _ = env.step(a) #execute step\n",
    "            Q[s,a] += alpha*(r + gamma * np.max(Q[nextState]) - Q[s,a])\n",
    "            s = nextState\n",
    "            rewards[e] += 1\n",
    "            lengths[e] = r\n",
    "            if d: #d true\n",
    "                break\n",
    "    for s in range(env.observation_space.n):\n",
    "        policy[s] = np.argmax(Q[s])\n",
    "    return policy, rewards, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes your implementation of *Q-Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  Cliff-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "\n",
      "Execution time: 0.2635s\n",
      "Policy:\n",
      "[['U' 'R' 'U' 'R' 'R' 'D' 'R' 'R' 'R' 'R' 'D' 'D']\n",
      " ['D' 'R' 'D' 'D' 'R' 'R' 'R' 'R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'D']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "envname = \"Cliff-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: \", envname)\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Q-Learning epsilon greedy\n",
    "policy, rewards, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Execution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "    env.shape))))\n",
    "_ = run_episode(env, policy, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for *Q-Learning* can be found [here](results/q_learning_results.txt). Notice that the result is stochastic so the final policy could differ a bit.\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "Your first assignment is to implement the *SARSA* algorithm on **Cliff**. In particular you need to implement both $\\epsilon$-greedy and *Softmax* versions for the exploration heuristic (you can reuse the same functions of Assignment 1). The solution returned must be a tuple *(policy, rewards, lengths)* where:\n",
    "* *policy*: array of action identifiers where the $i$-th action refers to the $i$-th state\n",
    "* *rewards*: array of rewards where the $i$-th reward refers to the $i$-th episode of the training performed\n",
    "* *lengths*: array of lengths where the $i$-th length refers to the $i$-th episode of the training performed (length in number of steps)\n",
    "\n",
    "Functions to implement:\n",
    "* *SARSA(environment, episodes, alpha, gamma, expl_func, expl_param)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    rewards = np.zeros(episodes)  # Rewards array\n",
    "    lengths = np.zeros(episodes)  # Lengths array\n",
    "    policy = np.random.choice(environment.action_space.n, environment.observation_space.n)  # Random policy\n",
    "    #Genero Q\n",
    "    Q = np.zeros((environment.observation_space.n,environment.action_space.n))\n",
    "    for e in range(episodes):\n",
    "        s = environment.reset()\n",
    "        a = expl_func(Q,s,expl_param)\n",
    "        while True:\n",
    "            nextState,r,d, _ = environment.step(a) #execute step\n",
    "            aFirst = expl_func(Q,nextState,expl_param)\n",
    "            Q[s,a] += alpha*(r + gamma * Q[nextState, aFirst] - Q[s,a])\n",
    "            s = nextState\n",
    "            aFirst = a\n",
    "            rewards[e] += 1\n",
    "            lengths[e] = r\n",
    "            if d: #d true\n",
    "                break\n",
    "    for s in range(environment.observation_space.n):\n",
    "        policy[s] = np.argmax(Q[s])\n",
    "    return policy, rewards, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes your implementation of *SARSA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  Cliff-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-cc65b9f379cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# SARSA epsilon greedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_greedy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m print(\"Execution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n\u001b[0;32m     22\u001b[0m     env.shape))))\n",
      "\u001b[1;32m<ipython-input-11-48c35a70c473>\u001b[0m in \u001b[0;36msarsa\u001b[1;34m(environment, episodes, alpha, gamma, expl_func, expl_param)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#execute step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0maFirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpl_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpl_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnextState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maFirst\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnextState\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-114bc016ad52>\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[1;34m(q, state, epsilon)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[0mbestAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mbestAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#best_action += rnd.randint(1,3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envname = \"Cliff-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: \", envname)\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# SARSA epsilon greedy\n",
    "policy, rews, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Execution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "    env.shape))))\n",
    "_ = run_episode(env, policy, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for *SARSA* can be found [here](results/sarsa_results.txt). Notice that the result is stochastic so the final policy could differ a bit.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Now that you have veryfied the results, try to employ *Softmax* instead of $\\epsilon$-greedy as exploration heuristic. Are there any significant changes? Why?\n",
    "\n",
    "## Comparison\n",
    "The following code performs a comparison between the 3 reinforcement learning algorithms of session 4 and 5: *Model-Based*, *Q-Learning* and *SARSA*. First of all copy your *value_iteration* and *model_based* functions from previous sessions here below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters, gamma, delta):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: max iterations allowed\n",
    "        gamma: gamma value\n",
    "        delta: delta value\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    v = np.zeros(environment.observation_space.n)  # Value function init\n",
    "    iters = 0\n",
    "    while True:\n",
    "        iters += 1\n",
    "        pv = v.copy()  # Previous value function\n",
    "        v = (environment.T * (environment.R + gamma * v)).sum(axis=2).max(axis=1)  # New value function: max(a) of Q(s, a)\n",
    "        # Check for convergence\n",
    "        if iters == maxiters or np.max(np.abs(v - pv)) < delta:\n",
    "            break\n",
    "    # Extract policy by argmax(a) of Q(s, a): best action for each state\n",
    "    return (environment.T * (environment.R + gamma * v)).sum(axis=2).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_based(environment, episodes, ep_limit, vmaxiters, gamma, delta):\n",
    "    \"\"\"\n",
    "    Performs the model-based algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        ep_limit: limit to episode length\n",
    "        vmaxiters: max iterations allowed for VI\n",
    "        gamma: gamma value\n",
    "        delta: delta value\n",
    "        \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    policy = np.random.choice(environment.action_space.n, environment.observation_space.n)  # Random initial policy\n",
    "    #policy = np.zeros(environment.observation_space.n, dtype=\"int16\")\n",
    "    # Counters\n",
    "    ct = np.zeros((environment.observation_space.n, environment.action_space.n, environment.observation_space.n))\n",
    "    cr = np.zeros((environment.observation_space.n, environment.action_space.n, environment.observation_space.n))\n",
    "    for i in range(episodes):\n",
    "        s = environment.reset()\n",
    "        iters = 0\n",
    "        while True:\n",
    "            sp, r, d, _ = environment.step(policy[s])  # Execute a step\n",
    "            # Update counters\n",
    "            ct[s, policy[s], sp] += 1\n",
    "            cr[s, policy[s], sp] += r\n",
    "            iters += 1\n",
    "            rews[i] += r\n",
    "            if d or iters == ep_limit:\n",
    "                break\n",
    "            s = sp\n",
    "        lengths[i] = iters\n",
    "        # Update transition model and rewards\n",
    "        environment.R = cr.copy()\n",
    "        np.divide(cr, ct, out=environment.R, where=ct != 0)\n",
    "        ts = ct.sum(axis=2, keepdims=True)\n",
    "        environment.T = ct.copy()\n",
    "        np.divide(ct, ts, out=environment.T, where=ts != 0)\n",
    "        policy = value_iteration(environment, vmaxiters, gamma, delta)  # Compute policy with VI\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following code and analyze the charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  Cliff-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "envname = \"Cliff-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: \", envname)\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "\n",
    "# Learning parameters\n",
    "episodes = 500\n",
    "ep_limit = 50\n",
    "vmaxiters = 50\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "delta = 1e-3\n",
    "\n",
    "rewser = []\n",
    "lenser = []\n",
    "\n",
    "litres = np.arange(1, episodes + 1)  # Learning iteration values\n",
    "window = 10  # Rolling window\n",
    "mrew = np.zeros(episodes)\n",
    "mlen = np.zeros(episodes)\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Model-based\n",
    "_, rews, lengths = model_based(env, episodes, ep_limit, vmaxiters, gamma, delta)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"label\": \"Model-Based\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"label\": \"Model-Based\"})\n",
    "\n",
    "# Q-Learning\n",
    "_, rews, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "\n",
    "# SARSA\n",
    "_, rews, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"label\": \"SARSA\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"label\": \"SARSA\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "\n",
    "plot(rewser, \"Rewards\", \"Episodes\", \"Rewards\")\n",
    "plot(lenser, \"Lengths\", \"Episodes\", \"Lengths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Can you explain the different trends in the charts?\n",
    "\n",
    "Correct trends can be seen in the following images: yours should be similar\n",
    "\n",
    "### Rewards\n",
    "![rewards](results/rewards.png)\n",
    "\n",
    "### Lengths\n",
    "![lengths](results/lengths.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
